---
title: '[Python] Logistic regression'
date: 2017-10-02
permalink: /posts/2017/10/python_logistic/
tags:
  - python
  - glm
  - logistic regression
---

In this short tutorial we will implement two different methods for finding the maximum likelihood estimate (MLE) of a logistic regression model ([McCullagh and Nelder, 1989](https://www.crcpress.com/Generalized-Linear-Models-Second-Edition/McCullagh-Nelder/p/book/9780412317606)), using Python. The MLE will be obtained using the algorithms:

* Iteratively Reweighted Least Squares (IRLS)
* Expectation Maximization (EM) via Polya-Gamma data augmentation


## Logistic regression

We start by describing the logistic regression model. Let $$\textbf{y} = (y_1,\dots,y_n)$$ be a vector of binary indicators depending on a set of covariates $$\textbf{x}_i \in \mathbb{R}^p$$, for $$i=1,\dots,n$$ so that
$$
\mathbb{P}(Y_i = 1) = \pi_i, \qquad \text{logit}(\pi_i) = \eta_i = \textbf{x}_i^T\beta,
$$
independently for $i=1,\dots,n$, where $$\beta \in \mathbb{R}^p$$. The logit function defined above is such that its inverse is equal to

$$\text{logit}^{-1}(\eta_i) = \frac{1}{1+\exp{(-\eta_i)}}.$$



### Iterated Weighted Least Square


```python
def invlogit(eta):
    return(1.0/(1.0+np.exp(-eta)))
```

Some fake data are simulated to check whether the algorithm is working properly. We assume that

$$\beta = [0,1]^T, \quad \text{logit}(\pi_i) = \beta_0 + \beta_1x_i, \quad y_i \sim \text{Bern}(\pi_i),$$

independently for $i=1,\dots,n$.

## Simulation study

```python
n = 50000
X = np.matrix([np.repeat(1,n),np.random.uniform(-2,2,n)]).T
beta = np.matrix([0,1]).T
pi = invlogit(X * beta)
y = np.matrix([x for x in map(lambda x : np.random.binomial(1,x),pi)]).T
```



```python
out = glm_logit(y,X)
out['Estimate']
```

    Convergence reached after 6 iterations





    matrix([[-0.00297888],
            [ 0.99732109]])



The standard error are also computed and reported here.


```python
out['Std error']
```




    matrix([[ 0.01025072,  0.00998682]])



We finnaly double check this results using the built-in function available in the package statsmodels. The estimates $\hat{\beta}$, fortunately, coincides.


```python
import statsmodels.api as sm

model = sm.GLM(y,X,family = sm.families.Binomial())
res = model.fit()
res.summary()
```




<table class="simpletable">
<caption>Generalized Linear Model Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>          <td>y</td>        <th>  No. Observations:  </th>  <td> 50000</td>
</tr>
<tr>
  <th>Model:</th>                 <td>GLM</td>       <th>  Df Residuals:      </th>  <td> 49998</td>
</tr>
<tr>
  <th>Model Family:</th>       <td>Binomial</td>     <th>  Df Model:          </th>  <td>     1</td>
</tr>
<tr>
  <th>Link Function:</th>        <td>logit</td>      <th>  Scale:             </th>    <td>1.0</td>  
</tr>
<tr>
  <th>Method:</th>               <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -28220.</td>
</tr>
<tr>
  <th>Date:</th>           <td>Fri, 23 Sep 2016</td> <th>  Deviance:          </th> <td>  56441.</td>
</tr>
<tr>
  <th>Time:</th>               <td>11:05:14</td>     <th>  Pearson chi2:      </th> <td>4.99e+04</td>
</tr>
<tr>
  <th>No. Iterations:</th>         <td>6</td>        <th>                     </th>     <td> </td>   
</tr>
</table>
<table class="simpletable">
<tr>
    <td></td>       <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th> <th>[95.0% Conf. Int.]</th>
</tr>
<tr>
  <th>const</th> <td>   -0.0030</td> <td>    0.010</td> <td>   -0.291</td> <td> 0.771</td> <td>   -0.023     0.017</td>
</tr>
<tr>
  <th>x1</th>    <td>    0.9973</td> <td>    0.010</td> <td>   99.864</td> <td> 0.000</td> <td>    0.978     1.017</td>
</tr>
</table>
